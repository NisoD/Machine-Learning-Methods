# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hKL5q7NXh2ixudP1yHLVuzhJUeYHmaYY
"""

!pip install tqdm
!pip install torchtext==0.6.0
!pip install spacy
!pythom -m spacy download en_core_web_sm

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch
import torchtext
import spacy
from torchtext.data import get_tokenizer
from torch.utils.data import random_split
from torchtext.experimental.datasets import IMDB
from torch.utils.data import DataLoader
from tqdm import tqdm
import torch.nn.functional as F
import os
import matplotlib.pyplot as plt


class MySelfAttention(nn.Module):
    """
    Self attention layer
    """

    def __init__(self, input_dim):
        """
        :param input_dim: The feature dimension the input tokens (d).
        """
        super(MySelfAttention, self).__init__()
        self.input_dim = input_dim
        # Query current token or element for which we are looking at the attention
        self.W_Q = nn.Linear(input_dim, input_dim)
        # Key other tokens or elements in the sequence
        self.W_K = nn.Linear(input_dim, input_dim)
        # Value actual information corresponding to the key
        self.W_V = nn.Linear(input_dim, input_dim)

    def forward(self, x):
        ### YOUR CODE HERE ###
        Q = self.W_Q(x)
        K = self.W_K(x)
        V = self.W_V(x)
        # Calculate the attention scores
        # made a bug when i ran without
        attention_scores = torch.matmul(Q, K.transpose(-2, -1))
        # normalize
        attention_scores /= self.input_dim**(0.5)
        # Softmax
        attention_scores = F.softmax(attention_scores, dim=-1)
        # Weighted sum of the values
        output = torch.matmul(attention_scores, V)
        return output


class MyLayerNorm(nn.Module):

    """
    Layer Normalization layer.
    """

    def __init__(self, input_dim):
        """
        :param input_dim: The dimension of the input (T, d).
        """
        super(MyLayerNorm, self).__init__()
        self.gamma = nn.Parameter(torch.ones(*input_dim))
        self.beta = nn.Parameter(torch.zeros(*input_dim))
        self.epsilon = 10 ** (-8)

    def forward(self, x):
        mean = torch.mean(x, dim=-1, keepdim=True)
        var = torch.var(x, dim=-1, keepdim=True)
        x_norm = (x - mean) / torch.sqrt(var + self.epsilon)
        x_out = x_norm * self.gamma + self.beta
        return x_out


class MyTransformerBlock(nn.Module):
    """
    Transformer block.
    """

    def __init__(self, max_len, input_dim):
        super(MyTransformerBlock, self).__init__()
        self.attention = MySelfAttention(input_dim)
        self.norm1 = MyLayerNorm((max_len, input_dim))
        self.norm2 = MyLayerNorm((max_len, input_dim))
        self.fc1 = nn.Linear(input_dim, input_dim)
        self.fc2 = nn.Linear(input_dim, input_dim)
        self.dropout = nn.Dropout(0.1)

    def forward(self, x):
        out = self.attention(x)
        x = self.norm1(self.dropout(out) + x)
        out = self.fc2(F.relu(self.fc1(x)))
        out = self.norm2(out + x)
        return out


class MyTransformer(nn.Module):
    """
    Transformer.
    """

    def __init__(self, vocab, max_len, num_of_blocks):
        """
        :param vocab: The vocabulary object.
        :param num_of_blocks: The number of transformer blocks.
        """
        super(MyTransformer, self).__init__()
        self.embedding = nn.Embedding.from_pretrained(vocab.vectors)
        self.emb_dim = self.embedding.embedding_dim
        self.max_len = max_len
        self.blocks = nn.ModuleList(
            [MyTransformerBlock(self.max_len, self.emb_dim) for _ in range(num_of_blocks)])
        self.fc = nn.Linear(self.emb_dim, 1)

    def forward(self, x):
        x = self.embedding(x)
        for block in self.blocks:
            x = block(x)
        avg_pooling = x.mean(dim=1)
        x = self.fc(avg_pooling)
        return x

def pad_trim(data):
    ''' Pads or trims the batch of input data.

    Arguments:
        data (torch.Tensor): input batch
    Returns:
        new_input (torch.Tensor): padded/trimmed input
        labels (torch.Tensor): batch of output target labels
    '''
    data = list(zip(*data))
    # Extract target output labels
    labels = torch.tensor(data[0]).float().to(device)
    # Extract input data
    inputs = data[1]

    # Extract only the part of the input up to the MAX_SEQ_LEN point
    # if input sample contains more than MAX_SEQ_LEN. If not then
    # select entire sample and append <pad_id> until the length of the
    # sequence is MAX_SEQ_LEN
    new_input = torch.stack([torch.cat((input[:MAX_SEQ_LEN],
                                        torch.tensor([pad_id] * max(0, MAX_SEQ_LEN - len(input))).long()))
                             for input in inputs])

    return new_input, labels

def split_train_val(train_set):
    ''' Splits the given set into train and validation sets WRT split ratio
    Arguments:
        train_set: set to split
    Returns:
        train_set: train dataset
        valid_set: validation dataset
    '''
    train_num = int(SPLIT_RATIO * len(train_set))
    valid_num = len(train_set) - train_num
    generator = torch.Generator().manual_seed(SEED)
    train_set, valid_set = random_split(train_set, lengths=[train_num, valid_num],
                                        generator=generator)
    return train_set, valid_set

def load_imdb_data():
    """
    This function loads the IMDB dataset and creates train, validation and test sets.
    It should take around 15-20 minutes to run on the first time (it downloads the GloVe embeddings, IMDB dataset and extracts the vocab).
    Don't worry, it will be fast on the next runs. It is recommended to run this function before you start implementing the training logic.
    :return: train_set, valid_set, test_set, train_loader, valid_loader, test_loader, vocab, pad_id
    """
    cwd = os.getcwd()
    if not os.path.exists(cwd + '/.vector_cache'):
        os.makedirs(cwd + '/.vector_cache')
    if not os.path.exists(cwd + '/.data'):
        os.makedirs(cwd + '/.data')
    # Extract the initial vocab from the IMDB dataset
    vocab = IMDB(data_select='train')[0].get_vocab()
    # Create GloVe embeddings based on original vocab word frequencies
    glove_vocab = torchtext.vocab.Vocab(counter=vocab.freqs,
                                        max_size=MAX_VOCAB_SIZE,
                                        min_freq=MIN_FREQ,
                                        vectors=torchtext.vocab.GloVe(name='6B'))
    # Acquire 'Spacy' tokenizer for the vocab words
    tokenizer = get_tokenizer('spacy', 'en_core_web_sm')
    # Acquire train and test IMDB sets with previously created GloVe vocab and 'Spacy' tokenizer
    train_set, test_set = IMDB(tokenizer=tokenizer, vocab=glove_vocab)
    vocab = train_set.get_vocab()  # Extract the vocab of the acquired train set
    pad_id = vocab['<pad>']  # Extract the token used for padding

    train_set, valid_set = split_train_val(train_set)  # Split the train set into train and validation sets

    train_loader = DataLoader(train_set, batch_size=batch_size, collate_fn=pad_trim)
    valid_loader = DataLoader(valid_set, batch_size=batch_size, collate_fn=pad_trim)
    test_loader = DataLoader(test_set, batch_size=batch_size, collate_fn=pad_trim)
    return train_set, valid_set, test_set, train_loader, valid_loader, test_loader, vocab, pad_id

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
# VOCAB AND DATASET HYPERPARAMETERS, DO NOT CHANGE
MAX_VOCAB_SIZE = 25000 # Maximum number of words in the vocabulary
MIN_FREQ = 10 # We include only words which occur in the corpus with some minimal frequency
MAX_SEQ_LEN = 500 # We trim/pad each sentence to this number of words
SPLIT_RATIO = 0.8 # Split ratio between train and validation set
SEED = 42

# YOUR HYPERPARAMETERS
### YOUR CODE HERE ###
batch_size = 32
num_of_blocks = 1
num_of_epochs = 5
learning_rate = 0.0001

# Load the IMDB dataset
train_set, valid_set, test_set, train_loader, valid_loader, test_loader, vocab, pad_id = load_imdb_data()

model = MyTransformer(vocab=vocab, max_len=MAX_SEQ_LEN, num_of_blocks=num_of_blocks).to(device)
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)
criterion = torch.nn.BCEWithLogitsLoss().to(device)
training_loss = []
validation_loss = []
accuracy = 0
### EXAMPLE CODE FOR RUNNING THE DATALOADER.
# for batch in tqdm(train_loader, desc='Train', total=len(train_loader)):
#     inputs_embeddings, labels = batch
for ep in range(num_of_epochs):
    #Train the model
    model.train()
    loss_train = 0
    for batch in tqdm(train_loader, desc='Train', total=len(train_loader)):
        inputs_embeddings, labels = batch
        inputs_embeddings = inputs_embeddings.to(device)
        labels = labels.to(device)

        optimizer.zero_grad()
        outputs = model(inputs_embeddings)
        loss = criterion(outputs.squeeze(), labels)
        loss.backward()
        optimizer.step()
        loss_train+=(loss.item())
    #Calculate the average loss for the epoch
    training_loss.append(loss_train/len(train_loader))

    #Test the model
    model.eval()
    correct = 0
    total = 0
    with torch.no_grad():
        for batch in tqdm(test_loader, desc=f'test Epoch {ep + 1}', total=len(valid_loader)):
            inputs_embeddings, labels = batch
            inputs_embeddings, labels = inputs_embeddings.to(device), labels.to(device)
            outputs = model(inputs_embeddings)
            preds = (outputs > 0.5).int()
            total += labels.size(0)
            correct += (preds.squeeze() == labels).sum().item()
        accuracy += correct/total


    # Validate the model
    model.eval()
    valid_loss = 0
    with torch.no_grad():
        for batch in tqdm(valid_loader, desc=f'Valid Epoch {ep + 1}', total=len(valid_loader)):
            inputs_embeddings, labels = batch
            inputs_embeddings, labels = inputs_embeddings.to(device), labels.to(device)
            outputs = model(inputs_embeddings)
            loss = criterion(outputs.squeeze(), labels)
            valid_loss += loss.item()
    validation_loss.append(valid_loss / len(valid_loader))

accuracy /= num_of_epochs


plt.plot(training_loss, label='Training Loss', linestyle='-', color='blue')
plt.plot(validation_loss, label='Validation Loss', linestyle='--', color='orange')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Training and Validation Losses')
plt.legend(loc='upper right')
plt.grid(True)
plt.show()

# Report test accuracy
print(f'Test Accuracy: {accuracy:.2f}')